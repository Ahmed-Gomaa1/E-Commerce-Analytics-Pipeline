# E-Commerce Analytics Pipeline with AWS Glue, Athena, and S3

## ğŸ“Œ Project Overview

This project implements a full serverless analytics pipeline for e-commerce user behavior tracking.&#x20;
The solution ingests raw event and product data into Amazon S3, processes it using AWS Glue (PySpark), and enables interactive querying via Amazon Athena.

It supports both historical batch processing and real-time extensibility.&#x20;
The output can feed dashboards, ML pipelines, or reports.

---

## ğŸ—‚ï¸ Data Sources

**Stored in S3 bucket:** `s3://ecommerce-data/`

### Raw Folder Structure

```
raw/
â”œâ”€â”€ events/               # Clickstream data: views, add-to-cart, transactions
â”œâ”€â”€ item_properties/      # Item property logs with time series values
â””â”€â”€ category_tree/        # Hierarchical category mappings
```

### Processed Folder Structure

```
processed/
â”œâ”€â”€ enriched_events/      # Events joined with item and category data (partitioned)
â”œâ”€â”€ item_properties/      # Cleaned, deduplicated item properties (latest only)
â””â”€â”€ category_hierarchy/   # Recursive tree of category_id paths
```

---

## ğŸ§ª ETL Job (AWS Glue Script)

The ETL job is built using PySpark via AWS Glue. It:

1. Loads events and item properties from the Glue Data Catalog.
2. Cleans & transforms timestamps and property values.
3. Deduplicates and pivots important properties like `price`, `available`, and `categoryid`.
4. Joins events with item metadata.
5. Builds recursive category hierarchies.
6. Writes Parquet files into partitioned output folders.

### Key Script Paths

```python
partitionKeys = ["event_type", "event_date"]  # Used for Athena partition discovery
```

Output format: `Parquet` with `Snappy` compression.

---

## ğŸ§­ Architecture Diagram

### ğŸ”§ Services Used

* **Amazon S3** â€“ Stores raw and processed data
* **AWS Glue** â€“ ETL processing using PySpark
* **Amazon Athena** â€“ Serverless SQL over processed data

## ğŸ” Athena Tables

Make sure your Glue job has written to partitioned folders, then run:

### 1. `processed_events`

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS processed_events (
  event_time TIMESTAMP,
  visitor_id BIGINT,
  item_id BIGINT,
  transaction_id BIGINT
)
PARTITIONED BY (
  event_type STRING,
  event_date STRING
)
STORED AS PARQUET
LOCATION 's3://my-ecommerce-data-2024/processed/enriched_events/';

MSCK REPAIR TABLE processed_events;
```

### 2. `processed_item_properties`

```sql
CREATE EXTERNAL TABLE processed_item_properties (
  property_time TIMESTAMP,
  item_id BIGINT,
  property_name STRING,
  property_value STRING,
  numeric_value FLOAT
)
STORED AS PARQUET
LOCATION 's3://my-ecommerce-data-2024/processed/item_properties/';
```

### 3. `processed_category_tree`

```sql
CREATE EXTERNAL TABLE processed_category_tree (
  category_id BIGINT,
  parent_category_id BIGINT,
  category_path ARRAY<BIGINT>
)
STORED AS PARQUET
LOCATION 's3://my-ecommerce-data-2024/processed/category_hierarchy/';
```

---

## ğŸ“Š Sample Queries

### ğŸ”¸ Events by Type

```sql
SELECT event_type, COUNT(*) FROM processed_events GROUP BY event_type;
```

### ğŸ”¸ Latest Item Properties

```sql
SELECT item_id, MAX(property_time) AS last_seen
FROM processed_item_properties
GROUP BY item_id;
```

### ğŸ”¸ Deepest Category Nodes

```sql
SELECT category_id, CARDINALITY(category_path) AS depth
FROM processed_category_tree
ORDER BY depth DESC
LIMIT 5;
```

## âœ… Project Highlights

* âœ… Scalable, serverless batch ETL architecture
* âœ… Automated partitioning for analytics
* âœ… Athena SQL support for analysts & BI tools
* âœ… Real-time extensible with Lambda or Kinesis

---

## ğŸ“ Folder Layout

```
S3
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ events/
â”‚   â”œâ”€â”€ item_properties/
â”‚   â””â”€â”€ category_tree/
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ enriched_events/
â”‚   â”œâ”€â”€ item_properties/
â”‚   â””â”€â”€ category_hierarchy/
â””â”€â”€ scripts/
    â””â”€â”€ etl_script.py
```

---

## ğŸ Final Thoughts

This architecture supports:

* Unified analytics and ML
* Scalable pipelines
* Extendability into Redshift, QuickSight, or SageMaker

> Built as part of a data engineering challenge â€“ designed for real-world e-commerce scale analytics.
